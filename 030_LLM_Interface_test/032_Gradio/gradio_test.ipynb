{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8250500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, ConfigurableField \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d593aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model choices\n",
    "MODEL_CHOICES = [\"gpt-4o\", \"gpt-4o-mini\", \"o3-mini\"]\n",
    "DEFAULT_MODEL = os.environ.get(\"OPENAI_MODEL_NAME\", \"gpt-4o\")\n",
    "\n",
    "# LangChain Implementation\n",
    "\n",
    "\n",
    "def create_chain(temperature, basemodel):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful AI assistant that provides detailed and accurate responses.\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    model = ChatOpenAI(\n",
    "        temperature=temperature,\n",
    "        model=basemodel,\n",
    "        streaming=True\n",
    "    )\n",
    "    chain = (\n",
    "        {\"input\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17690c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat history\n",
    "\n",
    "\n",
    "def respond(message, model_type, temperature, top_k, basemodel, history):\n",
    "    history = history or []\n",
    "    os.environ[\"OPENAI_MODEL_NAME\"] = basemodel\n",
    "    chain = create_chain(temperature, basemodel)\n",
    "    full_response = \"\"\n",
    "    for chunk in chain.stream(message):\n",
    "        full_response += chunk\n",
    "        yield \"\", history + [(message, full_response + \"â–Œ\")]\n",
    "    yield \"\", history + [(message, full_response)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc6f15a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yy/yc520yv550j7_tlzc44qpnsr0000gn/T/ipykernel_24236/2284910739.py:32: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"LLM Interface Test\") as demo:\n",
    "    gr.Markdown(\"# LLM Interface Test (LangChain Only)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            model_type = gr.Radio(\n",
    "                choices=[\"test1\", \"test2\", \"test3\"],\n",
    "                label=\"Select Model Type\",\n",
    "                value=\"test1\"\n",
    "            )\n",
    "            temperature = gr.Slider(\n",
    "                minimum=0.0,\n",
    "                maximum=2.0,\n",
    "                value=0.7,\n",
    "                step=0.05,\n",
    "                label=\"Temperature\"\n",
    "            )\n",
    "            top_k = gr.Slider(\n",
    "                minimum=1,\n",
    "                maximum=100,\n",
    "                value=40,\n",
    "                step=1,\n",
    "                label=\"Top-k (for sampling, if supported)\"\n",
    "            )\n",
    "            basemodel = gr.Dropdown(\n",
    "                choices=MODEL_CHOICES,\n",
    "                value=DEFAULT_MODEL if DEFAULT_MODEL in MODEL_CHOICES else MODEL_CHOICES[0],\n",
    "                label=\"Base Model (OPENAI_MODEL_NAME)\"\n",
    "            )\n",
    "\n",
    "    chatbot = gr.Chatbot(height=600)\n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(\n",
    "            show_label=False,\n",
    "            placeholder=\"Enter your message here\",\n",
    "            container=False\n",
    "        )\n",
    "        submit = gr.Button(\"Send\")\n",
    "\n",
    "    submit.click(respond, [msg, model_type, temperature, top_k, basemodel, chatbot], [\n",
    "                 msg, chatbot], api_name=\"chat\")\n",
    "    msg.submit(respond, [msg, model_type, temperature, top_k,\n",
    "               basemodel, chatbot], [msg, chatbot], api_name=\"chat\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.queue().launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e8dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "030_LLM_Interface_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
