{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feff62c2",
   "metadata": {},
   "source": [
    "# Crawl JAIS Papers by URL\n",
    "-  1. Extract URL from official website\n",
    "- 2. Add Title, Abstract, DOI, as a Customized class \"Paper\"\n",
    "\n",
    "> Used Libraries: Pandas, Beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5345b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA HANDLING\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# CRAWLING\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote\n",
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe59a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Paper:\n",
    "    title: str\n",
    "    abstract: str\n",
    "    citation: str\n",
    "    url: str\n",
    "    year: int\n",
    "    volume: int\n",
    "    issue: int\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Paper(title={self.title}, url={self.url}, year={self.year}, volume={self.volume}, issue={self.issue})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ffdb2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstract_crawler_by_url(issue=22, volume=1, num=1) -> Paper:\n",
    "    BASE_QUERY = \"https://aisel.aisnet.org/jais/\"\n",
    "    url = f\"{BASE_QUERY}/vol{volume}/iss{issue}/{num}/\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 1. ÎÖºÎ¨∏ Ï†úÎ™©\n",
    "        try:\n",
    "            title = soup.find(id='title').text.strip()\n",
    "        except AttributeError:\n",
    "            title = \"Unknown\"\n",
    "\n",
    "        # 2. ÏöîÏïΩ\n",
    "        try:\n",
    "            abstract = soup.find(id='abstract').text.strip()\n",
    "        except AttributeError:\n",
    "            abstract = \"Unknown\"\n",
    "\n",
    "        # 3. Ïù∏Ïö© Ï†ïÎ≥¥\n",
    "        try:\n",
    "            citation = soup.find(id='recommended_citation').text.strip()\n",
    "        except AttributeError:\n",
    "            citation = \"Unknown\"\n",
    "\n",
    "        # 4. Ïó∞ÎèÑ Í≥ÑÏÇ∞\n",
    "        year = volume - 1 + 2000\n",
    "\n",
    "        return Paper(\n",
    "            title=title,\n",
    "            abstract=abstract,\n",
    "            citation=citation,\n",
    "            url=url,\n",
    "            year=year,\n",
    "            volume=volume,\n",
    "            issue=issue\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error crawling {url}: {e}\")\n",
    "        return Paper(\n",
    "            title=\"Unknown\",\n",
    "            abstract=\"Unknown\",\n",
    "            citation=\"Unknown\",\n",
    "            url=url,\n",
    "            year=volume - 1 + 2000,\n",
    "            volume=volume,\n",
    "            issue=issue\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34531fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 23, issue 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vol 23 iss 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:07<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 23, issue 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vol 23 iss 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 23, issue 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "vol 23 iss 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:04<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 23, issue 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vol 23 iss 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:04<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 23, issue 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vol 23 iss 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 23, issue 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vol 23 iss 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:07<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 24, issue 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vol 24 iss 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:06<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 24, issue 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vol 24 iss 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 24, issue 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vol 24 iss 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 24, issue 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vol 24 iss 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 24, issue 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vol 24 iss 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:06<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 24, issue 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vol 24 iss 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 25, issue 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vol 25 iss 1: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 25, issue 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vol 25 iss 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:07<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 25, issue 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vol 25 iss 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:06<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 25, issue 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vol 25 iss 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:06<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 25, issue 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vol 25 iss 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:06<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 25, issue 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vol 25 iss 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:06<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 26, issue 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vol 26 iss 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:07<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling vol 26, issue 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vol 26 iss 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:09<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "vols = [23, 24, 25, 26]\n",
    "issues = [[1, 2, 3, 4, 5, 6],\n",
    "          [1, 2, 3, 4, 5, 6],\n",
    "          [1, 2, 3, 4, 5, 6],\n",
    "          [1, 2]]\n",
    "BASE_QUERY = \"https://aisel.aisnet.org/jais/\"\n",
    "papers = []\n",
    "\n",
    "for i, vol in enumerate(vols):\n",
    "    for issue in issues[i]:\n",
    "        print(f\"üîç Crawling vol {vol}, issue {issue}...\")\n",
    "        url = f\"{BASE_QUERY}vol{vol}/iss{issue}/\"\n",
    "        result = requests.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(result.text, 'html.parser')\n",
    "        docs = soup.select('h2#article ~ div.doc')\n",
    "        Article_num = len(docs)\n",
    "\n",
    "        for j in tqdm(range(Article_num), desc=f\"vol {vol} iss {issue}\", leave=True):\n",
    "            doc_url = url + f\"{j+1}/\"\n",
    "            paper = abstract_crawler_by_url(issue, vol, j+1)\n",
    "            papers.append(paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d447a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ ÌîÑÎ†àÏûÑÏúºÎ°ú Î≥ÄÌôò Î∞è CSV Ï†ÄÏû•\n",
    "papers_df = pd.DataFrame([paper.__dict__ for paper in papers])\n",
    "papers_df.to_csv('JAIS_papers.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa24952",
   "metadata": {},
   "source": [
    "## 2025ÎÖÑ PreprintÎ∂ÑÏóê ÎåÄÌï¥ÏÑú Ï∂îÍ∞ÄÎ°ú ÏàòÌñâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12d9b58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprints: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:07<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Paper(title=Data Control Coordination in the  Formation of Ecosystems in Highly Regulated Sectors, url=https://aisel.aisnet.org/jais_preprints/167, year=2025, volume=0, issue=Preprint), Paper(title=What Is Augmented? A Meta-Narrative Review of AI-Based Augmentation, url=https://aisel.aisnet.org/jais_preprints/168, year=2025, volume=0, issue=Preprint), Paper(title=Computationally Intensive Research:  Advancing a Role for Secondary Analysis of  Qualitative Data, url=https://aisel.aisnet.org/jais_preprints/169, year=2025, volume=0, issue=Preprint), Paper(title=Achieving Reward-Based Crowdfunding Project Success:  An Examination of Value Congruence, url=https://aisel.aisnet.org/jais_preprints/170, year=2025, volume=0, issue=Preprint), Paper(title=Achieving Reward-Based Crowdfunding Project Success:  An Examination of Value Congruence, url=https://aisel.aisnet.org/jais_preprints/171, year=2025, volume=0, issue=Preprint), Paper(title=Capturing the ‚ÄúSocial‚Äù in Social Networks: The Conceptualization and Empirical Application of Relational Quality, url=https://aisel.aisnet.org/jais_preprints/172, year=2025, volume=0, issue=Preprint), Paper(title=Polarization or Bias:  Take Your Click on Social Media, url=https://aisel.aisnet.org/jais_preprints/173, year=2025, volume=0, issue=Preprint), Paper(title=Polarization or Bias:  Take Your Click on Social Media, url=https://aisel.aisnet.org/jais_preprints/174, year=2025, volume=0, issue=Preprint), Paper(title=Empowering Marginalized Communities: A Framework  for Social Inclusion, url=https://aisel.aisnet.org/jais_preprints/175, year=2025, volume=0, issue=Preprint), Paper(title=Corporate Nomads: Working at the Boundary Between Corporate Work and Digital Nomadism, url=https://aisel.aisnet.org/jais_preprints/176, year=2025, volume=0, issue=Preprint), Paper(title=Why Do We Follow Virtual Influencer Recommendations? Three Theoretical Explanations from Brain Data Tested with Self-Reports, url=https://aisel.aisnet.org/jais_preprints/177, year=2025, volume=0, issue=Preprint), Paper(title=How Sentiment Divergence in Influencers‚Äô Multimodal Social Media Posts Shapes Follower Engagement?, url=https://aisel.aisnet.org/jais_preprints/178, year=2025, volume=0, issue=Preprint)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprints\n",
    "BASE_QUERY = \"https://aisel.aisnet.org/jais_preprints/\"\n",
    "papers = []\n",
    "\n",
    "for num in tqdm(range(167, 179), desc=f\"Preprints\", leave=True):\n",
    "    \n",
    "    url = f\"{BASE_QUERY}{num}/\"\n",
    "    \n",
    "    response = requests.get(url, timeout=5)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        title = soup.find(id='title').text.strip()\n",
    "    except AttributeError:\n",
    "        title = \"Unknown\"\n",
    "    try: \n",
    "        abstract = soup.find(id='abstract').text.strip()\n",
    "    except AttributeError:\n",
    "        abstract = \"Unknown\"\n",
    "    try:\n",
    "        citation = soup.find(id='recommended_citation').text.strip()\n",
    "    except AttributeError:\n",
    "        citation = \"Unknown\"\n",
    "\n",
    "    paper = Paper(\n",
    "        title=title,\n",
    "        abstract=abstract,\n",
    "        citation=citation,\n",
    "        url=BASE_QUERY + f'{num}',\n",
    "        year=2025,\n",
    "        volume=0,\n",
    "        issue=\"Preprint\"\n",
    "    )\n",
    "    papers.append(paper)\n",
    "\n",
    "print(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e45de9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([paper.__dict__ for paper in papers])\n",
    "df.to_csv('JAIS_preprints.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "df2 = pd.read_csv('JAIS_papers.csv', encoding='utf-8-sig')\n",
    "df2 = pd.concat([df2, df], ignore_index=True)\n",
    "df2.to_csv('JAIS_papers.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
